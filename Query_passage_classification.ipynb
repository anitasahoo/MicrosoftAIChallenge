{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8ACm7JOnoViT"
   },
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import requests\n",
    "\n",
    "def download_file_from_google_drive(id, destination):\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params = { 'id' : id }, stream = True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "            params = { 'id' : id, 'confirm' : token }\n",
    "            response = session.get(URL, params = params, stream = True)\n",
    "\n",
    "    save_response_content(response, destination)\n",
    "\n",
    "def get_confirm_token(response):\n",
    "    for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "\n",
    "    return None\n",
    "\n",
    "def save_response_content(response, destination):\n",
    "    CHUNK_SIZE = 32768\n",
    "\n",
    "    with open(destination, \"wb\") as f:\n",
    "      for chunk in response.iter_content(CHUNK_SIZE):\n",
    "        if chunk: # filter out keep-alive new chunks\n",
    "          f.write(chunk)\n",
    "file_id = '1nUXvuTgaICF7Um9NPgHH_hlVG8jMK9WX'\n",
    "destination = 'Data.tsv'\n",
    "download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "file_id = '1DhunfTzOTZHSTyOYLmIaVobQlLU_AG8u'\n",
    "destination = 'eval1_unlabelled.tsv'\n",
    "download_file_from_google_drive(file_id, destination)\n",
    "\n",
    "file_id = '1SRwaKKUE-x-deox5cPFntVvKYMu7BpEO'\n",
    "destination = 'glove.6B.50d.txt'\n",
    "download_file_from_google_drive(file_id, destination)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 275
    },
    "colab_type": "code",
    "id": "Oo0EyQNWolQs",
    "outputId": "5b626303-766f-4c34-87cb-3f173d4a6498"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "libopenmpi-dev is already the newest version (2.1.1-8).\n",
      "openmpi-bin is already the newest version (2.1.1-8).\n",
      "libopencv-dev is already the newest version (3.2.0+dfsg-4ubuntu0.1).\n",
      "python-opencv is already the newest version (3.2.0+dfsg-4ubuntu0.1).\n",
      "python3-opencv is already the newest version (3.2.0+dfsg-4ubuntu0.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 8 not upgraded.\n",
      "Requirement already satisfied: cntk-gpu in /usr/local/lib/python3.6/dist-packages (2.6)\n",
      "Requirement already satisfied: scipy>=0.17 in /usr/local/lib/python3.6/dist-packages (from cntk-gpu) (1.1.0)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from cntk-gpu) (1.14.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/cntk/cntk_py_init.py:56: UserWarning: Unsupported Linux distribution (ubuntu-18.04). CNTK supports Ubuntu 16.04 and above, only.\n",
      "  warnings.warn('Unsupported Linux distribution (%s-%s). CNTK supports Ubuntu 16.04 and above, only.' % (__my_distro__, __my_distro_ver__))\n"
     ]
    }
   ],
   "source": [
    "!apt-get install --no-install-recommends openmpi-bin libopenmpi-dev libopencv-dev python3-opencv python-opencv && ln -sf /usr/lib/x86_64-linux-gnu/libmpi_cxx.so /usr/lib/x86_64-linux-gnu/libmpi_cxx.so.1 && ln -sf /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so /usr/lib/x86_64-linux-gnu/openmpi/lib/libmpi.so.12 && ln -sf /usr/lib/x86_64-linux-gnu/libmpi.so /usr/lib/x86_64-linux-gnu/libmpi.so.12 && pip install cntk-gpu\n",
    "import cntk as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ppf4jjyTomty"
   },
   "outputs": [],
   "source": [
    "import cntk as C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uhocmPH2opsn"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import pickle\n",
    "\n",
    "\n",
    "\n",
    "#Initialize Global variables \n",
    "\n",
    "docIDFDict = {}\n",
    "\n",
    "avgDocLength = 0\n",
    "\n",
    "\n",
    "def GetCorpus(inputfile,trainfile,validfile):\n",
    "\n",
    "  f = open(inputfile,\"r\",encoding=\"utf-8\")\n",
    "\n",
    "  ft = open(trainfile,\"w\",encoding=\"utf-8\")\n",
    "\n",
    "  fv = open(validfile,\"w\",encoding=\"utf-8\")\n",
    "\n",
    "  i=0\n",
    "\n",
    "  for line in f:  \n",
    "\n",
    "                if (i<3000000):\n",
    "\n",
    "                    ft.write(line)\n",
    "\n",
    "                elif(i<50):\n",
    "\n",
    "                    fv.write(line)\n",
    "\n",
    "                i=i+1\n",
    "\n",
    "  ft.close()\n",
    "\n",
    "  fv.close()\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__' :\n",
    "\n",
    "\n",
    "\n",
    "    inputFileName = \"Data.tsv\"   # This file should be in the following format : queryid \\t query \\t passage \\t label \\t passageid\n",
    "\n",
    "    trainfile = 'traindata.tsv'\n",
    "\n",
    "    validfile = 'validationdata.tsv'\n",
    "\n",
    "    testFileName = \"eval1_unlabelled.tsv\"  # This file should be in the following format : queryid \\t query \\t passage \\t passageid # order of the query\n",
    "\n",
    "    #corpusFileName = \"corpus.tsv\" \n",
    "\n",
    "    #outputFileName = \"answer.tsv\"\n",
    "\n",
    "\n",
    "\n",
    "    GetCorpus(inputFileName,trainfile,validfile)    # Gets all the passages(docs) and stores in corpusFile. you can comment this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "jgazlAq6o0sw",
    "outputId": "2549017a-4522-44b0-9e9f-f1a6bfc079c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data conversion is done\n",
      "Validation Data conversion is done\n",
      "Evaluation Data conversion is done\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "#Initialize Global variables \n",
    "GloveEmbeddings = {}\n",
    "max_query_words = 12\n",
    "max_passage_words = 50\n",
    "emb_dim = 50\n",
    "#emb_dim = 300\n",
    "#The following method takes Glove Embedding file and stores all words and their embeddings in a dictionary\n",
    "def loadEmbeddings(embeddingfile):\n",
    "    global GloveEmbeddings,emb_dim\n",
    "\n",
    "    fe = open(embeddingfile,\"r\",encoding=\"utf-8\",errors=\"ignore\")\n",
    "    for line in fe:\n",
    "        tokens= line.strip().split()\n",
    "        word = tokens[0]\n",
    "        vec = tokens[1:]\n",
    "        vec = \" \".join(vec)\n",
    "        GloveEmbeddings[word]=vec\n",
    "    #Add Zerovec, this will be useful to pad zeros, it is better to experiment with padding any non-zero constant values also.\n",
    "    GloveEmbeddings[\"zerovec\"] = \"0.0 \"*emb_dim\n",
    "    fe.close()\n",
    "\n",
    "\n",
    "def TextDataToCTF(inputfile,outputfile,isEvaluation):\n",
    "    global GloveEmbeddings,emb_dim,max_query_words,max_passage_words\n",
    "\n",
    "    f = open(inputfile,\"r\",encoding=\"utf-8\",errors=\"ignore\")  # Format of the file : query_id \\t query \\t passage \\t label \\t passage_id\n",
    "    fw = open(outputfile,\"w\",encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        tokens = line.strip().lower().split(\"\\t\")\n",
    "        query_id,query,passage,label = tokens[0],tokens[1],tokens[2],tokens[3]\n",
    "\n",
    "        #****Query Processing****\n",
    "        words = re.split('\\W+', query)\n",
    "        words = [x for x in words if x] # to remove empty words \n",
    "        word_count = len(words)\n",
    "        remaining = max_query_words - word_count  \n",
    "        if(remaining>0):\n",
    "            words += [\"zerovec\"]*remaining # Pad zero vecs if the word count is less than max_query_words\n",
    "        words = words[:max_query_words] # trim extra words\n",
    "        #create Query Feature vector \n",
    "        query_feature_vector = \"\"\n",
    "        for word in words:\n",
    "            if(word in GloveEmbeddings):\n",
    "                query_feature_vector += GloveEmbeddings[word]+\" \"\n",
    "            else:\n",
    "                query_feature_vector += GloveEmbeddings[\"zerovec\"]+\" \"  #Add zerovec for OOV terms\n",
    "        query_feature_vector = query_feature_vector.strip() \n",
    "\n",
    "        #***** Passage Processing **********\n",
    "        words = re.split('\\W+', passage)\n",
    "        words = [x for x in words if x] # to remove empty words \n",
    "        word_count = len(words)\n",
    "        remaining = max_passage_words - word_count  \n",
    "        if(remaining>0):\n",
    "            words += [\"zerovec\"]*remaining # Pad zero vecs if the word count is less than max_passage_words\n",
    "        words = words[:max_passage_words] # trim extra words\n",
    "        #create Passage Feature vector \n",
    "        passage_feature_vector = \"\"\n",
    "        for word in words:\n",
    "            if(word in GloveEmbeddings):\n",
    "                passage_feature_vector += GloveEmbeddings[word]+\" \"\n",
    "            else:\n",
    "                passage_feature_vector += GloveEmbeddings[\"zerovec\"]+\" \"  #Add zerovec for OOV terms\n",
    "        passage_feature_vector = passage_feature_vector.strip() \n",
    "\n",
    "        #convert label\n",
    "        label_str = \" 1 0 \" if label==\"0\" else \" 0 1 \" \n",
    "\n",
    "        if(not isEvaluation):\n",
    "            fw.write(\"|qfeatures \"+query_feature_vector+\" |pfeatures \"+passage_feature_vector+\" |labels \"+label_str+\"\\n\")\n",
    "        else:\n",
    "            fw.write(\"|qfeatures \"+query_feature_vector+\" |pfeatures \"+passage_feature_vector+\"|qid \"+str(query_id)+\"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    trainFileName = \"traindata.tsv\"\n",
    "    validationFileName = \"validationdata.tsv\"\n",
    "    EvaluationFileName = \"eval1_unlabelled.tsv\"\n",
    "\n",
    "    embeddingFileName = \"glove.6B.50d.txt\"\n",
    "    #embeddingFileName = \"glove.6B.300d.txt\"\n",
    "    loadEmbeddings(embeddingFileName)    \n",
    "\n",
    "    # Convert Query,Passage Text Data to CNTK Text Format(CTF) using 50-Dimension Glove word embeddings \n",
    "    TextDataToCTF(trainFileName,\"TrainData.ctf\",False)\n",
    "    print(\"Train Data conversion is done\")\n",
    "    TextDataToCTF(validationFileName,\"ValidationData.ctf\",False)\n",
    "    print(\"Validation Data conversion is done\")\n",
    "    TextDataToCTF(EvaluationFileName,\"EvaluationData.ctf\",True)\n",
    "    print(\"Evaluation Data conversion is done\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yDrkoaTYo7JA"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import cntk as C\n",
    "from cntk.ops import combine, splice, sequence, reconcile_dynamic_axes\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs,INFINITELY_REPEAT\n",
    "from cntk.learners import sgd, learning_parameter_schedule_per_sample\n",
    "from cntk import input_variable, cross_entropy_with_softmax,classification_error, sequence\n",
    "from cntk.logging import ProgressPrinter\n",
    "from cntk.layers import Sequential, Embedding, Recurrence, LSTM, Dense\n",
    "from cntk.layers import Sequential\n",
    "from cntk.layers.typing import Tensor, Sequence\n",
    "from cntk.io import MinibatchSource, CTFDeserializer, StreamDef, StreamDefs, INFINITELY_REPEAT, FULL_DATA_SWEEP\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.exceptions import UndefinedMetricWarning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UndefinedMetricWarning) \n",
    "from google.colab import files\n",
    "\n",
    "\n",
    "#Initialize Global variables\n",
    "validation_query_vectors = []\n",
    "validation_passage_vectors = []\n",
    "validation_labels = []   \n",
    "q_max_words=12\n",
    "p_max_words=50\n",
    "emb_dim=50\n",
    "\n",
    "\n",
    "# The following LoadValidationSet method reads ctf format validation file and creates query, passage feature vectors and also copies labels for each pair.\n",
    "## the created vectors will be useful to find metrics on validation set after training each epoch which will be useful to decide the best model \n",
    "def LoadValidationSet(validationfile):\n",
    "    f = open(validationfile,'r',encoding=\"utf-8\")\n",
    "    for line in f:\n",
    "        tokens = line.strip().split(\"|\")  \n",
    "        #tokens[0] will be empty token since the line is starting with |\n",
    "        x1 = tokens[1].replace(\"qfeatures\",\"\").strip() #Query Features\n",
    "        x2 = tokens[2].replace(\"pfeatures\",\"\").strip() # Passage Features\n",
    "        y = tokens[3].replace(\"labels\",\"\").strip() # labels\n",
    "        x1 = [float(v) for v in x1.split()]\n",
    "        x2 = [float(v) for v in x2.split()]\n",
    "        y = [int(w) for w in y.split()]        \n",
    "        y = y[1] # label will be at index 1, i.e. if y = \"1 0\" then label=0 else if y=\"0 1\" then label=1\n",
    "\n",
    "        validation_query_vectors.append(x1)\n",
    "        validation_passage_vectors.append(x2)\n",
    "        validation_labels.append(y)\n",
    "\n",
    "        #print(\"1\")\n",
    "    \n",
    "    print(\"Validation Vectors are created\")\n",
    "\n",
    "def cnn_network(queryfeatures, passagefeatures,num_classes):\n",
    "        #*****Hyper-Parameters******\n",
    "    HIDDEN_DIM = 50 # LSTM dimension\n",
    "    DSSM_DIM = 25 # Dense layer dimension\n",
    "    NEGATIVE_SAMPLES = 5\n",
    "    DROPOUT_RATIO = 0.2\n",
    "    with C.layers.default_options(initial_state=0.1, pad = False,activation=C.elu ):\n",
    "            r1=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=False, name='r1')(queryfeatures)\n",
    "            rr1=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=True, name='rr1')(queryfeatures)\n",
    "            rr11=splice(rr1,r1)\n",
    "            #r1 = C.sequence.last(r1)\n",
    "            k1=C.layers.BatchNormalization(map_rank=1)(rr11)\n",
    "            d1=C.layers.Dense(50, activation=C.elu, name='d1')(k1)\n",
    "            a1=C.layers.Dropout(DROPOUT_RATIO, name='a1')(d1)\n",
    "#             r11=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=True, name='r11')(a1)\n",
    "#             #r1 = C.sequence.last(r1)\n",
    "#             k11=C.layers.BatchNormalization(map_rank=1)(r11)\n",
    "#             d11=C.layers.Dense(50, activation=C.elu, name='d11')(k11)\n",
    "#             a11=C.layers.Dropout(DROPOUT_RATIO, name='a11')(d11)\n",
    "#             r12=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=False, name='r12')(a11)\n",
    "#             #r1 = C.sequence.last(r1)\n",
    "#             k12=C.layers.BatchNormalization(map_rank=1)(r12)\n",
    "#             d12=C.layers.Dense(50, activation=C.elu, name='d12')(k12)\n",
    "#             a12=C.layers.Dropout(DROPOUT_RATIO, name='a12')(d12)\n",
    "#             r13=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=True, name='r13')(a12)\n",
    "#             #r1 = C.sequence.last(r1)\n",
    "#             k13=C.layers.BatchNormalization(map_rank=1)(r13)\n",
    "#             d13=C.layers.Dense(50, activation=C.elu, name='d13')(k13)\n",
    "#             a13=C.layers.Dropout(DROPOUT_RATIO, name='a13')(d13)\n",
    "#             r14=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=False, name='r14')(a13)\n",
    "#             #r1 = C.sequence.last(r1)\n",
    "#             k14=C.layers.BatchNormalization(map_rank=1)(r14)\n",
    "#             d14=C.layers.Dense(50, activation=C.elu, name='d14')(k14)\n",
    "#             a14=C.layers.Dropout(DROPOUT_RATIO, name='a14')(d14)\n",
    "#             #b1=C.layers.Dense(num_classes*num_classes, activation=C.softmax, name='b1')(a1)\n",
    "            #r4=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=True, name='r4')(d1)\n",
    "            #r4 = C.sequence.last(r4)\n",
    "            #d4=C.layers.Dense(DSSM_DIM, activation=C.elu, name='d4')(r4)\n",
    "            #a1=C.layers.Dropout(DROPOUT_RATIO, name='a1')(d4)\n",
    "            #b1=C.layers.Dense(num_classes*num_classes, activation=C.softmax, name='b1')(a1)\n",
    "            r2=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=False, name='r2')(passagefeatures)\n",
    "            rr2=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=True, name='rr2')(passagefeatures)\n",
    "            rr22=splice(rr2,r2)\n",
    "            k2=C.layers.BatchNormalization(map_rank=1)(rr22)\n",
    "            #r2 = C.sequence.last(r2)\n",
    "            d2=C.layers.Dense(50, activation=C.elu, name='d2')(k2)\n",
    "            a2=C.layers.Dropout(DROPOUT_RATIO, name='a2')(d2)\n",
    "#             r21=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=True, name='r21')(a2)\n",
    "#             k21=C.layers.BatchNormalization(map_rank=1)(r21)\n",
    "#             #r2 = C.sequence.last(r2)\n",
    "#             d21=C.layers.Dense(50, activation=C.elu, name='d21')(k21)\n",
    "#             a21=C.layers.Dropout(DROPOUT_RATIO, name='a21')(d21)\n",
    "            \n",
    "#             r22=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=False, name='r22')(a21)\n",
    "#             #r1 = C.sequence.last(r1)\n",
    "#             k22=C.layers.BatchNormalization(map_rank=1)(r22)\n",
    "#             d22=C.layers.Dense(50, activation=C.elu, name='d22')(k22)\n",
    "#             a22=C.layers.Dropout(DROPOUT_RATIO, name='a22')(d22)\n",
    "#             r23=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=True, name='r23')(a22)\n",
    "#             #r1 = C.sequence.last(r1)\n",
    "#             k23=C.layers.BatchNormalization(map_rank=1)(r23)\n",
    "#             d23=C.layers.Dense(50, activation=C.elu, name='d23')(k23)\n",
    "#             a23=C.layers.Dropout(DROPOUT_RATIO, name='a23')(d23)\n",
    "#             r24=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=False, name='r23')(a23)\n",
    "#             #r1 = C.sequence.last(r1)\n",
    "#             k24=C.layers.BatchNormalization(map_rank=1)(r24)\n",
    "#             d24=C.layers.Dense(50, activation=C.elu, name='d24')(k24)\n",
    "#             a24=C.layers.Dropout(DROPOUT_RATIO, name='a24')(d24)\n",
    "\n",
    "            mergeQP1 = C.element_times(a1,a2)\n",
    "            r3=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=False, name='r3')(mergeQP1)\n",
    "            r4=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=True, name='r4')(mergeQP1)\n",
    "            r5=splice(r4,r3)\n",
    "            k3=C.layers.BatchNormalization(map_rank=1)(r5)\n",
    "            d3=C.layers.Dense(25, activation=C.elu, name='d3')(k3)\n",
    "            a3=C.layers.Dropout(DROPOUT_RATIO, name='a3')(d3)\n",
    "#             r31=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=True, name='r31')(a3)\n",
    "#             k31=C.layers.BatchNormalization(map_rank=1)(r31)\n",
    "#             d31=C.layers.Dense(25, activation=C.elu, name='d31')(k31)\n",
    "#             a31=C.layers.Dropout(DROPOUT_RATIO, name='a31')(d31)\n",
    "#             r32=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=False, name='r32')(a31)\n",
    "#             k32=C.layers.BatchNormalization(map_rank=1)(r32)\n",
    "#             d32=C.layers.Dense(25, activation=C.elu, name='d32')(k32)\n",
    "#             a32=C.layers.Dropout(DROPOUT_RATIO, name='a32')(d32)\n",
    "#             r33=C.layers.Recurrence(C.layers.LSTM(shape=50), go_backwards=True, name='r33')(a32)\n",
    "#             k33=C.layers.BatchNormalization(map_rank=1)(r33)\n",
    "#             d33=C.layers.Dense(25, activation=C.elu, name='d33')(k33)\n",
    "#             a33=C.layers.Dropout(DROPOUT_RATIO, name='a33')(d33)\n",
    "#             #b2=C.layers.Dense(num_classes*num_classes, activation=C.softmax, name='b2')(a2)\n",
    "            #mergeQP     = C.element_times(b1,b2)\n",
    "            #model   = C.layers.Dense(num_classes, activation=C.softmax,name=\"overall\")(mergeQP)\n",
    "            model   = C.layers.Dense(num_classes, activation=C.softmax,name=\"overall\")(a3)\n",
    "            return model\n",
    "\n",
    "def create_reader(path, is_training, query_total_dim, passage_total_dim, label_total_dim):\n",
    "    return MinibatchSource(CTFDeserializer(path, StreamDefs( queryfeatures = StreamDef(field='qfeatures', shape=query_total_dim,is_sparse=False), \n",
    "                                                            passagefeatures = StreamDef(field='pfeatures', shape=passage_total_dim,is_sparse=False), \n",
    "                                                            labels   = StreamDef(field='labels', shape=label_total_dim,is_sparse=False)\n",
    "                                                            )), \n",
    "                           randomize=is_training, max_sweeps = INFINITELY_REPEAT if is_training else FULL_DATA_SWEEP)\n",
    "\n",
    "def TrainAndValidate(trainfile):\n",
    "\n",
    "    #*****Hyper-Parameters******\n",
    "    q_max_words= 12\n",
    "    p_max_words = 50\n",
    "    #emb_dim = 300\n",
    "    emb_dim = 50\n",
    "    hidden_dim = 50\n",
    "    num_classes = 2\n",
    "    minibatch_size = 250\n",
    "    epoch_size = 1500000 #No.of samples in training set\n",
    "    total_epochs = 500 #Total number of epochs to run\n",
    "    query_total_dim = q_max_words*emb_dim\n",
    "    label_total_dim = num_classes\n",
    "    passage_total_dim = p_max_words*emb_dim\n",
    "    EMB_DIM   = 50\n",
    "\n",
    "\n",
    "\n",
    "    #****** Create placeholders for reading Training Data  ***********\n",
    "    #query_input_var =  C.ops.input_variable((1,q_max_words,emb_dim),np.float32,is_sparse=False)\n",
    "    #passage_input_var =  C.ops.input_variable((1,p_max_words,emb_dim),np.float32,is_sparse=False)\n",
    "    query_input_var =  C.sequence.input_variable((1,q_max_words,emb_dim),np.float32,is_sparse=False)\n",
    "    passage_input_var =  C.sequence.input_variable((1,p_max_words,emb_dim),np.float32,is_sparse=False)\n",
    "    output_var = C.sequence.input_variable(num_classes,np.float32,is_sparse = False)\n",
    "    print(type(query_input_var))\n",
    "    train_reader = create_reader(trainfile, True, query_total_dim, passage_total_dim, label_total_dim)\n",
    "    input_map = { query_input_var : train_reader.streams.queryfeatures, passage_input_var:train_reader.streams.passagefeatures, output_var : train_reader.streams.labels}\n",
    "\n",
    "    # ********* Model configuration *******\n",
    "    model_output = cnn_network(query_input_var, passage_input_var, num_classes)\n",
    "    loss = C.binary_cross_entropy(model_output, output_var)\n",
    "    pe = C.classification_error(model_output, output_var)\n",
    "    lr_per_minibatch = C.learning_rate_schedule(0.03, C.UnitType.minibatch)   \n",
    "    learner = C.adagrad(model_output.parameters, lr=lr_per_minibatch)\n",
    "    progress_printer = C.logging.ProgressPrinter(tag='Training', num_epochs=total_epochs)\n",
    "\n",
    "    #************Create Trainer with model_output object, learner and loss parameters*************  \n",
    "    trainer = C.Trainer(model_output, (loss, pe), learner, progress_printer)\n",
    "    C.logging.log_number_of_parameters(model_output) ; print()\n",
    "\n",
    "    # **** Train the model in batchwise mode *****\n",
    "    for epoch in range(total_epochs):       # loop over epochs\n",
    "        print(\"Epoch : \",epoch)\n",
    "        sample_count = 0\n",
    "        while sample_count < epoch_size:  # loop over minibatches in the epoch\n",
    "            data = train_reader.next_minibatch(min(minibatch_size, epoch_size - sample_count), input_map=input_map) # fetch minibatch.\n",
    "            trainer.train_minibatch(data)        # training step\n",
    "            sample_count += data[output_var].num_samples   # count samples processed so far\n",
    "\n",
    "        trainer.summarize_training_progress()\n",
    "                \n",
    "        model_output.save(\"RNN_{}.dnn\".format(epoch)) # Save the model for every epoch\n",
    "        #files.download('RNN_{}.dnn'.format(epoch))\n",
    "                #*** Find metrics on validation set after every epoch ******#  (Note : you can skip doing this for every epoch instead to optimize the time, do it after every k epochs)\n",
    "            #*** Find metrics on validation set after every epoch ******#  (Note : you can skip doing this for every epoch instead to optimize the time, do it after every k epochs)\n",
    "        predicted_labels=[]    \n",
    "    return model_output\n",
    "\n",
    "## The following GetPredictionOnEvalSet method reads all query passage pair vectors from CTF file and does forward prop with trained model to get similarity score\n",
    "## after getting scores for all the pairs, the output will be written into submission file. \n",
    "def GetPredictionOnEvalSet(model,testfile,submissionfile):\n",
    "    global q_max_words,p_max_words,emb_dim\n",
    "\n",
    "    f = open(testfile,'r',encoding=\"utf-8\")\n",
    "    all_scores={} # Dictionary with key = query_id and value = array of scores for respective passages\n",
    "    for line in f:\n",
    "        tokens = line.strip().split(\"|\")  \n",
    "        #tokens[0] will be empty token since the line is starting with |\n",
    "        x1 = tokens[1].replace(\"qfeatures\",\"\").strip() #Query Features\n",
    "        x2 = tokens[2].replace(\"pfeatures\",\"\").strip() # Passage Features\n",
    "        query_id = tokens[3].replace(\"qid\",\"\").strip() # Query_id\n",
    "        x1 = [float(v) for v in x1.split()]\n",
    "        x2 = [float(v) for v in x2.split()]    \n",
    "        queryVec   = np.array(x1,dtype=\"float32\").reshape(1,q_max_words,emb_dim)\n",
    "        passageVec = np.array(x2,dtype=\"float32\").reshape(1,p_max_words,emb_dim)\n",
    "        score = model(queryVec,passageVec)[0][0][1] # do forward-prop on model to get score\n",
    "        if(query_id in all_scores):\n",
    "            all_scores[query_id].append(score)\n",
    "        else:\n",
    "            all_scores[query_id] = [score]\n",
    "    fw = open(submissionfile,\"w\",encoding=\"utf-8\")\n",
    "    for query_id in all_scores:\n",
    "        scores = all_scores[query_id]\n",
    "        scores_str = [str(sc) for sc in scores] # convert all scores to string values\n",
    "        scores_str = \"\\t\".join(scores_str) # join all scores in list to make it one string with  tab delimiter.  \n",
    "        fw.write(query_id+\"\\t\"+scores_str+\"\\n\")\n",
    "    fw.close()\n",
    "\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    trainSetFileName = \"TrainData.ctf\"\n",
    "    validationSetFileName = \"ValidationData.ctf\"\n",
    "    testSetFileName = \"EvaluationData.ctf\"\n",
    "    submissionFileName = \"answer.tsv\"\n",
    "    print(\"done\")\n",
    "    LoadValidationSet(validationSetFileName)    #Load Validation Query, Passage Vectors from Validation CTF File\n",
    "    model = TrainAndValidate(trainSetFileName) # Training and validation methods    \n",
    "    GetPredictionOnEvalSet(model,testSetFileName,submissionFileName) # Get Predictions on Evaluation Set\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Query_passage_classification.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
